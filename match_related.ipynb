{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import util\n",
    "from typing import List, Tuple, Union, Any"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    " def retrieve_top_k_matches(queries:list, corpus, eco_prod:list,k:int)-> tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Takes in list of queries/item_name, the list of ecoinvent product_name, topK value\n",
    "        This function performs a cosine similarity search between a list of query embeddings  and a list of corpus embeddings\n",
    "        returns topk matchs and scores\n",
    "        \"\"\"\n",
    "        top_k = k\n",
    "        results = []\n",
    "        scores=[]\n",
    "        for query in queries:\n",
    "            query_embedding = bi_encoder.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
    "            # We use cosine-similarity and torch.topk\n",
    "            cos_scores = util.cos_sim(query_embedding, corpus)[0]##https://github.com/UKPLab/sentence-transformers/blob/42ab80a122ad521a4f9055f5530a954d29d232ce/sentence_transformers/util.py#L23\n",
    "            top_results = torch.topk(cos_scores, k=top_k)#https://pytorch.org/docs/stable/generated/torch.topk.html\n",
    "\n",
    "            for score, idx in zip(top_results[0], top_results[1]):\n",
    "                #(eco_prod[idx], \"(Score: {:.4f})\".format(score))\n",
    "                results.append(eco_prod[idx])\n",
    "                scores.append(float(\"(Score: {:.4f})\".format(score)[8:14]))\n",
    "        if top_k > 1:\n",
    "            results=[results[i:i+k] for i in range(0, len(results), k)]\n",
    "            scores=[scores[i:i+k] for i in range(0, len(scores), k)]\n",
    "        return (results, scores)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "test_1323 = retrieve_top_k_matches('zinc monosulfate', corpus_embeddings, corpus, k=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "[['zirconium oxide', 'pyrazole', 'zinc monosulfate', 'zinc oxide', 'pyridine'],\n ['aniline',\n  'adipic acid',\n  'benzimidazole-compound',\n  'indium',\n  'phenyl isocyanate'],\n ['calcium nitrate',\n  'sodium nitrate',\n  'sodium nitrite',\n  'nitrogen, liquid',\n  'potassium nitrate'],\n ['steel, unalloyed',\n  'hexane',\n  'vinyl carbonate',\n  'ascorbic acid',\n  'petroleum coke'],\n ['propyl amine', 'allyl chloride', 'propane', '1-propanol', 'acrolein'],\n ['lithium manganese oxide',\n  'N-methyl-2-pyrrolidone',\n  'melamine',\n  'N,N-dimethylformamide',\n  'chloromethyl methyl ether'],\n ['o-dichlorobenzene',\n  'o-chlorotoluene',\n  'diphenylether-compound',\n  'dioxane',\n  'benzyl chloride'],\n ['calcium nitrate',\n  'sodium nitrate',\n  'sodium nitrite',\n  'nitrogen, liquid',\n  'potassium nitrate'],\n ['o-dichlorobenzene',\n  'o-chlorotoluene',\n  'diphenylether-compound',\n  'dioxane',\n  'benzyl chloride'],\n ['sodium sulfate, anhydrite',\n  'dimethyl sulfate',\n  'sulfuryl chloride',\n  'potassium sulfate',\n  'calcium borates'],\n ['calcium chloride',\n  'p-dichlorobenzene',\n  'benzyl chloride',\n  'benzoic acid',\n  '2,4-dichlorotoluene'],\n ['lithium chloride',\n  'calcium chloride',\n  'potassium chloride',\n  'benzyl chloride',\n  'aluminium chloride'],\n ['diborane',\n  'carbon tetrachloride',\n  'steel, unalloyed',\n  'hexane',\n  'fluorine, liquid'],\n ['citric acid',\n  'fatty acid',\n  'propionic acid',\n  'acrolein',\n  'anthranilic acid'],\n ['purified terephthalic acid',\n  'polyethylene terephthalate, granulate, amorphous',\n  'phthalimide-compound',\n  'phthalimide',\n  'terbium oxide'],\n ['hexane', 'ethyl benzene', 'diborane', 'xylene', 'vinyl chloride']]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1323[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "bi_encoder = SentenceTransformer(\"ronanki/all-mpnet-base-v2-2022-11-07\")\n",
    "\n",
    "with open('/Users/avinashronanki/PycharmProjects/machine_learning/Emitter_Search/recommendations/model_6_prod.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    corpus = stored_data['sentences']\n",
    "    corpus_embeddings = stored_data['embeddings']\n",
    "\n",
    "def search(query, top_k=5):\n",
    "    results = []\n",
    "    # scores =[]\n",
    "    ##### Sematic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    # print(hits)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "    # score = hits[1]\n",
    "    for idx, hit in enumerate(hits[0:5]):\n",
    "        results.append(corpus[hit['corpus_id']])\n",
    "        # scores.append(score)\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    " def negative_creation(queries:list,eco_prod:list,k:int)-> Tuple[List[list]]:\n",
    "        \"\"\"\n",
    "        it is same as the match function but appends only the nth value\n",
    "        suggested k values are 6 10 12 'hard negatives'\n",
    "        Takes in list of queries/item_name, the list of ecoinvent product_name, Nth value\n",
    "        This function performs a cosine similarity search between a list of query embeddings  and a list of corpus embeddings\n",
    "        returns nth matchs and scores\n",
    "        \"\"\"\n",
    "        top_k = k\n",
    "        results = []\n",
    "        scores=[]\n",
    "        for query in queries:\n",
    "            query_embedding = bi_encoder.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
    "            # We use cosine-similarity and torch.topk\n",
    "            cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0] #https://github.com/UKPLab/sentence-transformers/blob/42ab80a122ad521a4f9055f5530a954d29d232ce/sentence_transformers/util.py#L23\n",
    "            top_results = torch.topk(cos_scores, k=top_k)#https://pytorch.org/docs/stable/generated/torch.topk.html\n",
    "\n",
    "            for score, idx in zip(top_results[0], top_results[1]):\n",
    "                (eco_prod[idx], \"(Score: {:.4f})\".format(score))\n",
    "            results.append(eco_prod[idx])\n",
    "            scores.append(float(\"(Score: {:.4f})\".format(score)[8:14]))\n",
    "        return (results,scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(['pyridine',\n  'phenyl isocyanate',\n  'potassium nitrate',\n  'petroleum coke',\n  'acrolein',\n  'chloromethyl methyl ether',\n  'benzyl chloride',\n  'potassium nitrate',\n  'benzyl chloride',\n  'calcium borates',\n  '2,4-dichlorotoluene',\n  'aluminium chloride',\n  'fluorine, liquid',\n  'anthranilic acid',\n  'terbium oxide',\n  'vinyl chloride'],\n [0.3463,\n  0.2226,\n  0.3693,\n  0.2346,\n  0.268,\n  0.3054,\n  0.2456,\n  0.3693,\n  0.2456,\n  0.3103,\n  0.2745,\n  0.3704,\n  0.2264,\n  0.2606,\n  0.2584,\n  0.2833])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_creation('zinc monosulfate',corpus, k=5 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Any\n",
    "\n",
    "def vlookup(df1: pd.DataFrame, col1: str, df2: pd.DataFrame, col2: str, output_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs a vlookup-like operation on two dataframes and returns a new dataframe with the specified columns.\n",
    "\n",
    "    Args:\n",
    "        df1: The first dataframe.\n",
    "        col1: The name of the column in df1 to merge on.\n",
    "        df2: The second dataframe.\n",
    "        col2: The name of the column in df2 to merge on.\n",
    "        output_col: The name of the column from df2 to include in the output.\n",
    "\n",
    "    Returns:\n",
    "        A new dataframe with the specified columns.\n",
    "    \"\"\"\n",
    "    # Perform a merge operation on the two dataframes\n",
    "    merged_df = pd.merge(df1, df2[[col2, output_col]], left_on=col1, right_on=col2, how='left')\n",
    "\n",
    "    # Drop the duplicate column and rename the output column\n",
    "    merged_df = merged_df.drop(col2, axis=1).rename(columns={output_col: f'{col1}_{output_col}'})\n",
    "\n",
    "    return merged_df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_df = vlookup(df1, 'ID', df2, 'Key', 'Value')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data, label_column, test_size=0.2, eval_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into train, test, and eval sets using stratified sampling.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The data to split.\n",
    "        label_column (str): The name of the column containing the labels.\n",
    "        test_size (float): The proportion of the data to use for testing.\n",
    "        eval_size (float): The proportion of the data to use for evaluation.\n",
    "        random_state (int): The random seed to use for the train-test split.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The data with an additional column \"split\" containing \"train\", \"test\", or \"eval\" for each row.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the data into train and test sets, stratifying on the label column\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        data.drop(columns=[label_column]), data[label_column], test_size=test_size, stratify=data[label_column], random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Split the remaining data into eval and test sets, stratifying on the label column\n",
    "    eval_data, test_data, eval_labels, test_labels = train_test_split(\n",
    "        test_data.drop(columns=[label_column]), test_labels, test_size=eval_size / (1 - test_size), stratify=test_labels, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Add a \"split\" column to each set\n",
    "    train_data[\"split\"] = \"train\"\n",
    "    test_data[\"split\"] = \"test\"\n",
    "    eval_data[\"split\"] = \"eval\"\n",
    "\n",
    "    # Combine the sets back into a single dataframe\n",
    "    split_data = pd.concat([train_data, test_data, eval_data], axis=0)\n",
    "\n",
    "    return split_data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: nltk in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (3.5)\r\n",
      "Requirement already satisfied: joblib in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\r\n",
      "Requirement already satisfied: regex in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\r\n",
      "Requirement already satisfied: tqdm in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.64.1)\r\n",
      "Requirement already satisfied: click in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting gensim\r\n",
      "  Using cached gensim-4.3.0-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\r\n",
      "Collecting smart-open>=1.8.1\r\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\r\n",
      "Collecting scipy>=1.7.0\r\n",
      "  Using cached scipy-1.10.1-cp38-cp38-macosx_10_9_x86_64.whl (35.0 MB)\r\n",
      "Collecting FuzzyTM>=0.4.0\r\n",
      "  Using cached FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.22.4)\r\n",
      "Collecting pyfume\r\n",
      "  Using cached pyFUME-0.2.25-py3-none-any.whl (67 kB)\r\n",
      "Requirement already satisfied: pandas in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from FuzzyTM>=0.4.0->gensim) (1.2.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2019.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim) (1.15.0)\r\n",
      "Collecting simpful\r\n",
      "  Downloading simpful-2.10.0-py3-none-any.whl (31 kB)\r\n",
      "Collecting fst-pso\r\n",
      "  Using cached fst_pso-1.8.1-py3-none-any.whl\r\n",
      "Collecting miniful\r\n",
      "  Using cached miniful-0.0.6-py3-none-any.whl\r\n",
      "Requirement already satisfied: requests in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.25.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (4.0.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.4)\r\n",
      "Installing collected packages: scipy, miniful, simpful, fst-pso, pyfume, smart-open, FuzzyTM, gensim\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.6.2\r\n",
      "    Uninstalling scipy-1.6.2:\r\n",
      "      Successfully uninstalled scipy-1.6.2\r\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 gensim-4.3.0 miniful-0.0.6 pyfume-0.2.25 scipy-1.10.1 simpful-2.10.0 smart-open-6.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/avinashronanki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/avinashronanki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/your/model'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstopwords\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Load the pre-trained word2vec model\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m w2v_model \u001B[38;5;241m=\u001B[39m \u001B[43mWord2Vec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpath/to/your/model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhybrid_search\u001B[39m(query, documents, num_results\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m):\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m# Tokenize the query and remove stopwords\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     query_tokens \u001B[38;5;241m=\u001B[39m [token\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m nltk\u001B[38;5;241m.\u001B[39mword_tokenize(query) \u001B[38;5;28;01mif\u001B[39;00m token\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m nltk\u001B[38;5;241m.\u001B[39mcorpus\u001B[38;5;241m.\u001B[39mstopwords\u001B[38;5;241m.\u001B[39mwords(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/gensim/models/word2vec.py:1942\u001B[0m, in \u001B[0;36mWord2Vec.load\u001B[0;34m(cls, rethrow, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1923\u001B[0m \u001B[38;5;124;03m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001B[39;00m\n\u001B[1;32m   1924\u001B[0m \n\u001B[1;32m   1925\u001B[0m \u001B[38;5;124;03mSee Also\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1939\u001B[0m \n\u001B[1;32m   1940\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1941\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1942\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mWord2Vec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1943\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, Word2Vec):\n\u001B[1;32m   1944\u001B[0m         rethrow \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/gensim/utils.py:486\u001B[0m, in \u001B[0;36mSaveLoad.load\u001B[0;34m(cls, fname, mmap)\u001B[0m\n\u001B[1;32m    482\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m object from \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, fname)\n\u001B[1;32m    484\u001B[0m compress, subname \u001B[38;5;241m=\u001B[39m SaveLoad\u001B[38;5;241m.\u001B[39m_adapt_by_suffix(fname)\n\u001B[0;32m--> 486\u001B[0m obj \u001B[38;5;241m=\u001B[39m \u001B[43munpickle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    487\u001B[0m obj\u001B[38;5;241m.\u001B[39m_load_specials(fname, mmap, compress, subname)\n\u001B[1;32m    488\u001B[0m obj\u001B[38;5;241m.\u001B[39madd_lifecycle_event(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloaded\u001B[39m\u001B[38;5;124m\"\u001B[39m, fname\u001B[38;5;241m=\u001B[39mfname)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/gensim/utils.py:1460\u001B[0m, in \u001B[0;36munpickle\u001B[0;34m(fname)\u001B[0m\n\u001B[1;32m   1446\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munpickle\u001B[39m(fname):\n\u001B[1;32m   1447\u001B[0m     \u001B[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001B[39;00m\n\u001B[1;32m   1448\u001B[0m \n\u001B[1;32m   1449\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1458\u001B[0m \n\u001B[1;32m   1459\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1460\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m   1461\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _pickle\u001B[38;5;241m.\u001B[39mload(f, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/smart_open/smart_open_lib.py:188\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transport_params \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    186\u001B[0m     transport_params \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 188\u001B[0m fobj \u001B[38;5;241m=\u001B[39m \u001B[43m_shortcut_open\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m    \u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbuffering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuffering\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnewline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fobj\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/smart_open/smart_open_lib.py:361\u001B[0m, in \u001B[0;36m_shortcut_open\u001B[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001B[0m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m    359\u001B[0m     open_kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124merrors\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m errors\n\u001B[0;32m--> 361\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_builtin_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuffering\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mopen_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'path/to/your/model'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the pre-trained word2vec model\n",
    "w2v_model = Word2Vec.load(\"path/to/your/model\")\n",
    "def hybrid_search(query, documents, num_results=10, threshold=0.5):\n",
    "    # Tokenize the query and remove stopwords\n",
    "    query_tokens = [token.lower() for token in nltk.word_tokenize(query) if token.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "    # Calculate the semantic similarity between the query and each document using the pre-trained word2vec model\n",
    "    semantic_similarities = []\n",
    "    for doc in documents:\n",
    "        doc_tokens = [token.lower() for token in nltk.word_tokenize(doc) if token.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "        similarity = w2v_model.wv.n_similarity(query_tokens, doc_tokens)\n",
    "        semantic_similarities.append(similarity)\n",
    "\n",
    "    # Filter the documents based on keyword and semantic similarity\n",
    "    relevant_docs = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        keyword_similarity = sum([1 for token in query_tokens if token in doc.lower()]) / len(query_tokens)\n",
    "        if keyword_similarity > threshold or semantic_similarities[i] > threshold:\n",
    "            relevant_docs.append((doc, keyword_similarity, semantic_similarities[i]))\n",
    "\n",
    "    # Sort the relevant documents by their combined keyword and semantic similarity scores\n",
    "    relevant_docs = sorted(relevant_docs, key=lambda x: x[1] + x[2], reverse=True)[:num_results]\n",
    "\n",
    "    # Return the top N relevant documents\n",
    "    return [doc[0] for doc in relevant_docs]\n",
    "query = \"How to train a machine learning model\"\n",
    "documents = [\n",
    "    \"A beginner's guide to machine learning\",\n",
    "    \"The importance of data preprocessing in machine learning\",\n",
    "    \"Training a machine learning model with scikit-learn\",\n",
    "    \"Deep learning for natural language processing\"\n",
    "]\n",
    "\n",
    "results = hybrid_search(query, documents)\n",
    "print(results)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: rank-bm25 in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (0.2.2)\r\n",
      "Requirement already satisfied: numpy in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from rank-bm25) (1.22.4)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence-transformers\n",
    "# !pip install rank-bm25\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/avinashronanki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/avinashronanki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'fastbm25' object has no attribute 'get_scores'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [44]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     36\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHow to train a machine learning model\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     37\u001B[0m documents \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA beginner\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms guide to machine learning\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe importance of data preprocessing in machine learning\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining a machine learning model with scikit-learn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDeep learning for natural language processing\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     42\u001B[0m ]\n\u001B[0;32m---> 44\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mhybrid_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28mprint\u001B[39m(results)\n",
      "Input \u001B[0;32mIn [44]\u001B[0m, in \u001B[0;36mhybrid_search\u001B[0;34m(query, documents, num_results, keyword_weight)\u001B[0m\n\u001B[1;32m     21\u001B[0m semantic_similarities \u001B[38;5;241m=\u001B[39m (query_embedding \u001B[38;5;241m@\u001B[39m doc_embeddings\u001B[38;5;241m.\u001B[39mT)\u001B[38;5;241m.\u001B[39mflatten()\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Calculate the BM25 scores between the query and each document using keyword matching\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m bm25_scores \u001B[38;5;241m=\u001B[39m \u001B[43mfastbm25\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_scores\u001B[49m(query_tokens)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Combine the keyword and semantic similarity scores\u001B[39;00m\n\u001B[1;32m     27\u001B[0m combined_scores \u001B[38;5;241m=\u001B[39m [(i, keyword_weight \u001B[38;5;241m*\u001B[39m bm25_scores[i] \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m keyword_weight) \u001B[38;5;241m*\u001B[39m semantic_similarities[i]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(documents))]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'fastbm25' object has no attribute 'get_scores'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the pre-trained sentence transformer model\n",
    "sent_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "\n",
    "def hybrid_search(query, documents, num_results=10, keyword_weight=0.5):\n",
    "    # Tokenize the query and remove stopwords\n",
    "    query_tokens = [token.lower() for token in nltk.word_tokenize(query) if token.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "    # Generate sentence embeddings for the query and documents\n",
    "    query_embedding = sent_model.encode(query, convert_to_tensor=True)\n",
    "    doc_embeddings = sent_model.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate the cosine similarity between the query and each document using sentence embeddings\n",
    "    semantic_similarities = (query_embedding @ doc_embeddings.T).flatten().tolist()\n",
    "\n",
    "    # Calculate the BM25 scores between the query and each document using keyword matching\n",
    "    bm25_scores = BM25Okapi(documents).get_scores(query_tokens)\n",
    "\n",
    "    # Combine the keyword and semantic similarity scores\n",
    "    combined_scores = [(i, keyword_weight * bm25_scores[i] + (1 - keyword_weight) * semantic_similarities[i]) for i in range(len(documents))]\n",
    "\n",
    "    # Sort the relevant documents by their combined scores\n",
    "    relevant_docs = sorted(combined_scores, key=lambda x: x[1], reverse=True)[:num_results]\n",
    "\n",
    "    # Return the top N relevant documents\n",
    "    return [documents[i] for i, _ in relevant_docs]\n",
    "\n",
    "\n",
    "query = \"How to train a machine learning model\"\n",
    "documents = [\n",
    "    \"A beginner's guide to machine learning\",\n",
    "    \"The importance of data preprocessing in machine learning\",\n",
    "    \"Training a machine learning model with scikit-learn\",\n",
    "    \"Deep learning for natural language processing\"\n",
    "]\n",
    "\n",
    "results = hybrid_search(query, documents)\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['a', \"beginner's\", 'guide', 'to', 'machine', 'learning'], 0, 1.19), (['training', 'a', 'machine', 'learning', 'model', 'with', 'scikit-learn'], 2, 1.1099999999999999), (['the', 'importance', 'of', 'data', 'preprocessing', 'in', 'machine', 'learning'], 1, 0.26)]\n"
     ]
    }
   ],
   "source": [
    "from fastbm25 import fastbm25\n",
    "\n",
    "query = \"How to train a machine learning model\"\n",
    "documents = [\n",
    "    \"A beginner's guide to machine learning\",\n",
    "    \"The importance of data preprocessing in machine learning\",\n",
    "    \"Training a machine learning model with scikit-learn\",\n",
    "    \"Deep learning for natural language processing\"\n",
    "]\n",
    "tokenized_corpus = [doc.lower().split(\" \") for doc in documents]\n",
    "model = fastbm25(tokenized_corpus)\n",
    "query = query.lower().split()\n",
    "result = model.top_k_sentence(query,k=3)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bm25'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# from fastbm25 import FastBM25\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbm25\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fastbm25 \u001B[38;5;28;01mas\u001B[39;00m BM25\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Download the necessary NLTK data\u001B[39;00m\n\u001B[1;32m      6\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpunkt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'bm25'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# from fastbm25 import FastBM25\n",
    "import nltk\n",
    "from bm25 import fastbm25 as BM25\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the pre-trained sentence transformer model\n",
    "sent_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "def hybrid_search(query, documents, num_results=10, keyword_weight=0.5):\n",
    "    # Tokenize the query and remove stopwords\n",
    "    query_tokens = [token.lower() for token in nltk.word_tokenize(query) if token.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "    # Generate sentence embeddings for the query and documents\n",
    "    query_embedding = sent_model.encode(query, convert_to_tensor=True)\n",
    "    doc_embeddings = sent_model.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate the cosine similarity between the query and each document using sentence embeddings\n",
    "    semantic_similarities = (query_embedding @ doc_embeddings.T).flatten().tolist()\n",
    "\n",
    "    # Calculate the BM25 scores between the query and each document using keyword matching\n",
    "    bm25 = FastBM25(documents)\n",
    "    bm25_scores = BM25(documents).get_scores(query_tokens)\n",
    "\n",
    "    # Combine the keyword and semantic similarity scores\n",
    "    combined_scores = [(i, keyword_weight * bm25_scores[i] + (1 - keyword_weight) * semantic_similarities[i]) for i in range(len(documents))]\n",
    "\n",
    "    # Sort the relevant documents by their combined scores\n",
    "    relevant_docs = sorted(combined_scores, key=lambda x: x[1], reverse=True)[:num_results]\n",
    "\n",
    "    # Return the top N relevant documents\n",
    "    return [documents[i] for i, _ in relevant_docs]\n",
    "\n",
    "query = \"How to train a machine learning model\"\n",
    "documents = [\n",
    "    \"A beginner's guide to machine learning\",\n",
    "    \"The importance of data preprocessing in machine learning\",\n",
    "    \"Training a machine learning model with scikit-learn\",\n",
    "    \"Deep learning for natural language processing\"\n",
    "]\n",
    "\n",
    "results = hybrid_search(query, documents)\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: rank-bm25 in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (0.2.2)\r\n",
      "Requirement already satisfied: numpy in /Users/avinashronanki/opt/anaconda3/lib/python3.8/site-packages (from rank-bm25) (1.22.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade rank-bm25"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rank_bm25'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrank_bm25\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BM25Okapi\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'rank_bm25'"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}